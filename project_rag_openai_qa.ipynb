{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbFcH36e26hP"
      },
      "source": [
        "# Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UREYtyXA2vtN",
        "outputId": "d02baf90-1f92-4b69-b1ec-f2a35e326beb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-openai langchain-core langchain_community docx2txt pypdf langchain_chroma sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylciedb03G9X"
      },
      "source": [
        "# Open AI API Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf4VyYOg3eTL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] ='<YOUR_OPEN_API_KEY>' #@param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-YT-PFK3Nn1"
      },
      "source": [
        "# Langchain and LangSmith using API Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebcuIq-A3XFg"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "\n",
        "os.environ['LANGCHAIN_API_KEY'] = 'your_langchain_api_key' #@param\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'PROJECT_RAG_OPENAI_QA'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHC4eSO_4C2H"
      },
      "source": [
        "# Testing LLM and Langchain Connections using simple LLM CALL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR0VzO0f4kv-"
      },
      "source": [
        "You should see the api calls in OpenAI API usage and Langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRK0Swpc4Q8R",
        "outputId": "d2a8915e-af0f-44f1-a08b-ae926af2f31f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 11, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'id': 'chatcmpl-BDjl5JcrdlfNDGbcYRoOQbMCyBtXZ', 'finish_reason': 'stop', 'logprobs': None}, id='run-f4cd328a-dc86-49c7-9fa2-a7d5ccc9bf78-0', usage_metadata={'input_tokens': 11, 'output_tokens': 14, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
        "\n",
        "llm_response = llm.invoke([\"Tell me a joke\"])\n",
        "\n",
        "llm_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUH6w3bi48eT"
      },
      "source": [
        "# Creating custom prompt templete using System Message and Human Message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yRmzUyS4x9i",
        "outputId": "0956e1e7-8d47-4633-aae9-f47176cd088f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that tells jokes.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about cars', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Create a template using ChatPromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = ChatPromptTemplate(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
        "        (\"human\", \"Tell me about {topic}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_value = template.invoke({\"topic\": \"cars\"})\n",
        "\n",
        "prompt_value\n",
        "\n",
        "#template.invoke({\"topic\": \"cars\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-m0qqFR5eTz"
      },
      "source": [
        "Invoke the LLM ( Send the prompt to LLM )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IznLHeFf5nu_",
        "outputId": "434868be-2de6-4020-937c-151337fccee2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Cars are fascinating machines! They come in all shapes and sizes, from compact hatchbacks to spacious SUVs. They get us from point A to point B, but they can also be the source of some great jokes. \\n\\nFor instance: \\n\\nWhy did the car break up with its girlfriend? \\n\\nBecause it couldn't handle all the “emotional exhaust”! \\n\\nIf you have any specific questions about cars, or if you want more jokes, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 24, 'total_tokens': 117, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'id': 'chatcmpl-BDjrqXD6U0iKgnIeS7q4z4P6Pu1kX', 'finish_reason': 'stop', 'logprobs': None}, id='run-fb7de685-ad4b-4c4b-911c-615f7d8efeb9-0', usage_metadata={'input_tokens': 24, 'output_tokens': 93, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(prompt_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsl7ZMk8jx2",
        "outputId": "82958c5a-494e-4858-b11c-d1ed616304d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.11/dist-packages (0.8)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.26.4)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.31.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2025.1.31)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Collecting eval-type-backport>=0.2.0 (from unstructured-client->unstructured)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (2.10.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Collecting typing-inspection>=0.4.0 (from unstructured-client->unstructured)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->unstructured-client->unstructured) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->unstructured-client->unstructured) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Downloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.31.3-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.8/175.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=5cfd6026379c25a9d239e9fb61b77e0587522bbada395fae747bcc939164e046\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, typing-inspection, rapidfuzz, python-magic, python-iso639, olefile, langdetect, eval-type-backport, emoji, aiofiles, python-oxmsg, unstructured-client, unstructured\n",
            "Successfully installed aiofiles-24.1.0 emoji-2.14.1 eval-type-backport-0.2.2 filetype-1.2.0 langdetect-1.0.9 olefile-0.47 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.12.2 typing-inspection-0.4.0 unstructured-0.17.2 unstructured-client-0.31.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf docx2txt unstructured"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LK2V9yv8Z_h"
      },
      "source": [
        "# LOAD, SPLIT, EMBED, STORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFsr1H9z8zN4",
        "outputId": "e7a6c1ec-45e1-45f9-de9e-a2d9fb44c0be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no of Processed files: 1\n",
            "no of Processed files: 2\n",
            "Number of documents pages loaded: 16\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.schema import Document\n",
        "from typing import List\n",
        "from re import split\n",
        "\n",
        "\n",
        "# simple function to load pdf and docx\n",
        "\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    \"\"\"Loads PDF and DOCX documents from a folder and returns a list of Documents.\"\"\"\n",
        "    documents = []\n",
        "    processed_files = set()\n",
        "    # Iterate over files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Check file type and load accordingly\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(file_path)  # Load PDF files\n",
        "        elif filename.endswith(\".docx\"):\n",
        "            loader = Docx2txtLoader(file_path)  # Load DOCX files\n",
        "        else:\n",
        "            print(f\"Skipping unsupported file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        # Load document content and append it to the list\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "        processed_files.add(filename)  # Store the filename\n",
        "\n",
        "        print(f\"no of Processed files: {len(processed_files)}\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "# Define the folder path where documents are stored\n",
        "folder_path = \"/content/docs\"\n",
        "\n",
        "# Load documents\n",
        "documents = load_documents(folder_path)\n",
        "\n",
        "\n",
        "print(f\"Number of documents pages loaded: {len(documents)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxNVxrqA-rJD"
      },
      "source": [
        "SPLIT THE DOCS INTO CHUNKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSLbqjCe-uTf",
        "outputId": "a0397505-7cd1-45ac-802b-35348d39ac2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of text chunks created: 37\n"
          ]
        }
      ],
      "source": [
        "# Create text splitter with chunking configuration\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "# split the document into the chunks\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Number of text chunks created: {len(splits)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "qq4NhhAC-21z",
        "outputId": "33dd208f-22d0-4885-e3d2-26fe9d1893b6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'arXiv:2412.15605v1  [cs.CL]  20 Dec 2024\\nDon’t Do RAG:\\nWhen Cache-Augmented Generation is All You Need for\\nKnowledge Tasks\\nBrian J Chan ∗\\nChao-Ting Chen∗\\nJui-Hung Cheng ∗\\nDepartment of Computer Science\\nNational Chengchi University\\nTaipei, Taiwan\\n{110703065,110703038,110703007}@nccu.edu.tw\\nHen-Hsen Huang\\nInsititue of Information Science\\nAcademia Sinica\\nTaipei, Taiwan\\nhhhuang@iis.sinica.edu.tw\\nAbstract\\nRetrieval-augmented generation (RAG) has gained tractionas a\\npowerful approach for enhancing language models by integra ting\\nexternal knowledge sources. However, RAG introduces chall enges\\nsuch as retrieval latency, potential errors in document sel ection,\\nand increased system complexity. With the advent of large la n-\\nguage models (LLMs) featuring signiﬁcantly extended conte xt win-\\ndows, this paper proposes an alternative paradigm, cache-a ugmented\\ngeneration (CAG) that bypasses real-time retrieval. Our me thod in-\\nvolves preloading all relevant resources, especially when the docu-\\nments or knowledge for retrieval are of a limited and managea ble\\nsize, into the LLM’s extended context and caching its runtim e pa-\\nrameters. During inference, the model utilizes these prelo aded pa-\\nrameters to answer queries without additional retrieval st eps. Com-\\nparative analyses reveal that CAG eliminates retrieval lat ency and\\nminimizes retrieval errors while maintaining context rele vance. Per-\\nformance evaluations across multiple benchmarks highligh t sce-\\nnarios where long-context LLMs either outperform or comple ment\\ntraditional RAG pipelines. These ﬁndings suggest that, for certain\\napplications, particularly those with a constrained knowl edge base,\\nCAG provide a streamlined and eﬃcient alternative to RAG, ac hiev-\\ning comparable or superior results with reduced complexity .\\nCCS Concepts\\n•Computing methodologies → Discourse, dialogue and prag-\\nmatics; Natural language generation ; • Information systems\\n→ Specialized information retrieval .\\nKeywords'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "3LFFByhK--jT",
        "outputId": "dde54ba2-8fba-4b44-d5bd-419b56c0c8e4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits[36].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M4QSpyJ_ouP"
      },
      "source": [
        "# EMBEDDING USING OPENAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rjdf_B_j_rZ3",
        "outputId": "92b127cb-e661-471d-f930-46454ee1a22b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of document embeddings: 37\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "document_embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
        "\n",
        "\n",
        "print(f\"Number of document embeddings: {len(document_embeddings)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKQiLysY_70X"
      },
      "source": [
        "# STORE THE EMBEDDED VECTORS INTO CHROMEDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2pluFnFABe7",
        "outputId": "5b76f63e-7bc1-4ff0-b00a-d3e0508cac70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-1c3ad9eb3a28>:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding_function = OpenAIEmbeddings()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector is stored in ./chroma_db\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Initialize OpenAI Embeddings\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "\n",
        "# Define collection name and persistence directory\n",
        "collection_name = \"my_collection\"\n",
        "persist_directory = \"./chroma_db\"  # Directory to store the vectors\n",
        "\n",
        "# Create Chroma vector store\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,  # List of text chunks (Ensure 'splits' is populated)\n",
        "    embedding=embedding_function,\n",
        "    persist_directory=persist_directory  # Persistent storage\n",
        ")\n",
        "\n",
        "print(f\"Vector is stored in {persist_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3m0M7TBAJd_"
      },
      "source": [
        "# SIMILARITY SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOIZg-plAPZp",
        "outputId": "dbbe48b3-d776-4c18-fd9d-fefc4025ba26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result 1:\n",
            "Source :/content/docs/CAG.pdf\n",
            "paper aims to provide insights into when and why CAG may serve\n",
            "as a streamlined, eﬀective alternative to RAG, particularly for cases\n",
            "where the documents or knowledge for retrieval are of limite d,\n",
            "manageable size. Our ﬁndings challenge the default relianc e on\n",
            "RAG for knowledge integration tasks, oﬀering a simpliﬁed, r obust\n",
            "solution to harness the growing capabilities of long-conte xt LLMs.\n",
            "Our contributions are threefold as follows:\n",
            "• Retrieval-Free Long-Context Paradigm: Introduced a novel\n",
            "approach leveraging long-context LLMs with preloaded doc-\n",
            "uments and precomputed KV caches, eliminating retrieval\n",
            "latency, errors, and system complexity.\n",
            "• Performance Comparison: Conducted extensive experiments\n",
            "showing scenarios where long-context LLMs outperform tra-\n",
            "ditional RAG systems, especially with manageable knowl-\n",
            "edge bases.\n",
            "• Practical Insights: Provided actionable insights into opt imiz-\n",
            "ing knowledge-intensive workﬂows, demonstrating the via-\n",
            "bility of retrieval-free methods for speciﬁc applications. Our\n",
            "CAG framework is released publicly. 1\n",
            "2 Methodology\n",
            "Our CAG framework leverages the extended context capabilities of\n",
            "long-context LLMs to enable retrieval-free knowledge inte gration.\n",
            "By preloading external knowledge sources, such as a collect ion\n",
            "of documents D = { /u1D4511, /u1D4512, . . .} , and precomputing the key-value\n",
            "(KV) cache CKV, we address the computational challenges and in-\n",
            "eﬃciencies inherent to real-time retrieval in traditional RAG sys-\n",
            "tems. The operation of our framework is divided into three ph ases:\n",
            "1https://github.com/hhhuang/CAG\n",
            "(1) External Knowledge Preloading\n",
            "In this phase, a curated collection of documentsD relevant\n",
            "to the target application is preprocessed and formatted to\n",
            "ﬁt within the model’s extended context window. The LLM\n",
            "M, with parameters /u1D703, processes D, transforming it into a\n",
            "precomputed KV cache:\n",
            "CKV = KV-Encode(D) (1)\n",
            "This KV cache, which encapsulates the inference state of\n",
            "Result 2:\n",
            "Source :/content/docs/CAG.pdf\n",
            "CCS Concepts\n",
            "•Computing methodologies → Discourse, dialogue and prag-\n",
            "matics; Natural language generation ; • Information systems\n",
            "→ Specialized information retrieval .\n",
            "Keywords\n",
            "Large Language Models, Retrieval Augmented Generation, Retrieval-\n",
            "Free Question Answering\n",
            "1 Introduction\n",
            "The advent of retrieval-augmented generation (RAG) [1, 3] h as\n",
            "signiﬁcantly enhanced the capabilities of large language m odels\n",
            "(LLMs) by dynamically integrating external knowledge sour ces. RAG\n",
            "systems have proven eﬀective in handling open-domain quest ions\n",
            "and specialized tasks, leveraging retrieval pipelines to p rovide con-\n",
            "textually relevant answers. However, RAG is not without its draw-\n",
            "backs. The need for real-time retrieval introduces latency , while\n",
            "∗Three authors contributed equally to this research.\n",
            "Knowledge\n",
            "Retrieval\n",
            "Model\n",
            "Q1\n",
            "K1\n",
            "Q2\n",
            "K2\n",
            "LLM\n",
            "LLM\n",
            "Q1K1\n",
            "A1\n",
            "Q2K2\n",
            "A2\n",
            "LLM\n",
            "Knowledge\n",
            "Cache\n",
            "Pre-compute\n",
            "Q1 Q2\n",
            "LLM\n",
            "Knowledge\n",
            "Cache\n",
            "Q2\n",
            "Q1\n",
            "Append Q1\n",
            "A1\n",
            "LLM\n",
            "Knowledge\n",
            "Cache Q2\n",
            "Truncate Q1\n",
            "Append Q2\n",
            "A2\n",
            "Figure 1: Comparison of Traditional RAG and our CAG\n",
            "Workﬂows: The upper section illustrates the RAG pipeline,\n",
            "including real-time retrieval and reference text input dur -\n",
            "ing inference, while the lower section depicts our CAG ap-\n",
            "proach, which preloads the KV-cache, eliminating the re-\n",
            "trieval step and reference text input at inference.\n",
            "errors in selecting or ranking relevant documents can degrade the\n",
            "quality of the generated responses. Additionally, integra ting re-\n",
            "trieval and generation components increases system comple xity,\n",
            "necessitating careful tuning and adding to the maintenance over-\n",
            "head.\n",
            "This paper proposes an alternative paradigm, cache-augmen ted\n",
            "generation (CAG), leveraging the capabilities of long-con text LLMs\n",
            "to address these challenges. Instead of relying on a retriev al pipeline,\n",
            "as shown in Figure 1, our approach involves preloading the LL M\n",
            "with all relevant documents in advance and precomputing the key-\n",
            "value (KV) cache, which encapsulates the inference state of the\n"
          ]
        }
      ],
      "source": [
        "# Similazrity search in vector db\n",
        "query = \"what is CAG?\"\n",
        "search_results = vectorstore.similarity_search(query,k=2)\n",
        "search_results\n",
        "\n",
        "for i , result in enumerate(search_results):\n",
        "  print(f\"Result {i+1}:\")\n",
        "  print(f\"Source :{result.metadata.get('source','Unknown')}\" )\n",
        "  print(result.page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6sOIfy0A5qB",
        "outputId": "c7ecf861-d713-4fb4-f0f2-250d77962c6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'creationdate': 'D:20250321224934', 'creator': 'PDFium', 'page': 1, 'page_label': '2', 'producer': 'PDFium', 'source': '/content/docs/CAG.pdf', 'total_pages': 5}, page_content='paper aims to provide insights into when and why CAG may serve\\nas a streamlined, eﬀective alternative to RAG, particularly for cases\\nwhere the documents or knowledge for retrieval are of limite d,\\nmanageable size. Our ﬁndings challenge the default relianc e on\\nRAG for knowledge integration tasks, oﬀering a simpliﬁed, r obust\\nsolution to harness the growing capabilities of long-conte xt LLMs.\\nOur contributions are threefold as follows:\\n• Retrieval-Free Long-Context Paradigm: Introduced a novel\\napproach leveraging long-context LLMs with preloaded doc-\\numents and precomputed KV caches, eliminating retrieval\\nlatency, errors, and system complexity.\\n• Performance Comparison: Conducted extensive experiments\\nshowing scenarios where long-context LLMs outperform tra-\\nditional RAG systems, especially with manageable knowl-\\nedge bases.\\n• Practical Insights: Provided actionable insights into opt imiz-\\ning knowledge-intensive workﬂows, demonstrating the via-\\nbility of retrieval-free methods for speciﬁc applications. Our\\nCAG framework is released publicly. 1\\n2 Methodology\\nOur CAG framework leverages the extended context capabilities of\\nlong-context LLMs to enable retrieval-free knowledge inte gration.\\nBy preloading external knowledge sources, such as a collect ion\\nof documents D = { /u1D4511, /u1D4512, . . .} , and precomputing the key-value\\n(KV) cache CKV, we address the computational challenges and in-\\neﬃciencies inherent to real-time retrieval in traditional RAG sys-\\ntems. The operation of our framework is divided into three ph ases:\\n1https://github.com/hhhuang/CAG\\n(1) External Knowledge Preloading\\nIn this phase, a curated collection of documentsD relevant\\nto the target application is preprocessed and formatted to\\nﬁt within the model’s extended context window. The LLM\\nM, with parameters /u1D703, processes D, transforming it into a\\nprecomputed KV cache:\\nCKV = KV-Encode(D) (1)\\nThis KV cache, which encapsulates the inference state of'),\n",
              " Document(metadata={'creationdate': 'D:20250321224934', 'creator': 'PDFium', 'page': 0, 'page_label': '1', 'producer': 'PDFium', 'source': '/content/docs/CAG.pdf', 'total_pages': 5}, page_content='CCS Concepts\\n•Computing methodologies → Discourse, dialogue and prag-\\nmatics; Natural language generation ; • Information systems\\n→ Specialized information retrieval .\\nKeywords\\nLarge Language Models, Retrieval Augmented Generation, Retrieval-\\nFree Question Answering\\n1 Introduction\\nThe advent of retrieval-augmented generation (RAG) [1, 3] h as\\nsigniﬁcantly enhanced the capabilities of large language m odels\\n(LLMs) by dynamically integrating external knowledge sour ces. RAG\\nsystems have proven eﬀective in handling open-domain quest ions\\nand specialized tasks, leveraging retrieval pipelines to p rovide con-\\ntextually relevant answers. However, RAG is not without its draw-\\nbacks. The need for real-time retrieval introduces latency , while\\n∗Three authors contributed equally to this research.\\nKnowledge\\nRetrieval\\nModel\\nQ1\\nK1\\nQ2\\nK2\\nLLM\\nLLM\\nQ1K1\\nA1\\nQ2K2\\nA2\\nLLM\\nKnowledge\\nCache\\nPre-compute\\nQ1 Q2\\nLLM\\nKnowledge\\nCache\\nQ2\\nQ1\\nAppend Q1\\nA1\\nLLM\\nKnowledge\\nCache Q2\\nTruncate Q1\\nAppend Q2\\nA2\\nFigure 1: Comparison of Traditional RAG and our CAG\\nWorkﬂows: The upper section illustrates the RAG pipeline,\\nincluding real-time retrieval and reference text input dur -\\ning inference, while the lower section depicts our CAG ap-\\nproach, which preloads the KV-cache, eliminating the re-\\ntrieval step and reference text input at inference.\\nerrors in selecting or ranking relevant documents can degrade the\\nquality of the generated responses. Additionally, integra ting re-\\ntrieval and generation components increases system comple xity,\\nnecessitating careful tuning and adding to the maintenance over-\\nhead.\\nThis paper proposes an alternative paradigm, cache-augmen ted\\ngeneration (CAG), leveraging the capabilities of long-con text LLMs\\nto address these challenges. Instead of relying on a retriev al pipeline,\\nas shown in Figure 1, our approach involves preloading the LL M\\nwith all relevant documents in advance and precomputing the key-\\nvalue (KV) cache, which encapsulates the inference state of the')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\":2})\n",
        "\n",
        "retriever.invoke(\"what is CAG?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqvcM3NSCBmN"
      },
      "source": [
        "# Converting Content into Single Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2oBOeOdBJPG"
      },
      "outputs": [],
      "source": [
        "def docs2str(docs):\n",
        "  return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV6w8GuMBPDs",
        "outputId": "226cd361-e186-437e-c726-f0fe511516c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content=\" Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know,\\npaper aims to provide insights into when and why CAG may serve\\nas a streamlined, eﬀective alternative to RAG, particularly for cases\\nwhere the documents or knowledge for retrieval are of limite d,\\nmanageable size. Our ﬁndings challenge the default relianc e on\\nRAG for knowledge integration tasks, oﬀering a simpliﬁed, r obust\\nsolution to harness the growing capabilities of long-conte xt LLMs.\\nOur contributions are threefold as follows:\\n• Retrieval-Free Long-Context Paradigm: Introduced a novel\\napproach leveraging long-context LLMs with preloaded doc-\\numents and precomputed KV caches, eliminating retrieval\\nlatency, errors, and system complexity.\\n• Performance Comparison: Conducted extensive experiments\\nshowing scenarios where long-context LLMs outperform tra-\\nditional RAG systems, especially with manageable knowl-\\nedge bases.\\n• Practical Insights: Provided actionable insights into opt imiz-\\ning knowledge-intensive workﬂows, demonstrating the via-\\nbility of retrieval-free methods for speciﬁc applications. Our\\nCAG framework is released publicly. 1\\n2 Methodology\\nOur CAG framework leverages the extended context capabilities of\\nlong-context LLMs to enable retrieval-free knowledge inte gration.\\nBy preloading external knowledge sources, such as a collect ion\\nof documents D = { /u1D4511, /u1D4512, . . .} , and precomputing the key-value\\n(KV) cache CKV, we address the computational challenges and in-\\neﬃciencies inherent to real-time retrieval in traditional RAG sys-\\ntems. The operation of our framework is divided into three ph ases:\\n1https://github.com/hhhuang/CAG\\n(1) External Knowledge Preloading\\nIn this phase, a curated collection of documentsD relevant\\nto the target application is preprocessed and formatted to\\nﬁt within the model’s extended context window. The LLM\\nM, with parameters /u1D703, processes D, transforming it into a\\nprecomputed KV cache:\\nCKV = KV-Encode(D) (1)\\nThis KV cache, which encapsulates the inference state of\\n\\nCCS Concepts\\n•Computing methodologies → Discourse, dialogue and prag-\\nmatics; Natural language generation ; • Information systems\\n→ Specialized information retrieval .\\nKeywords\\nLarge Language Models, Retrieval Augmented Generation, Retrieval-\\nFree Question Answering\\n1 Introduction\\nThe advent of retrieval-augmented generation (RAG) [1, 3] h as\\nsigniﬁcantly enhanced the capabilities of large language m odels\\n(LLMs) by dynamically integrating external knowledge sour ces. RAG\\nsystems have proven eﬀective in handling open-domain quest ions\\nand specialized tasks, leveraging retrieval pipelines to p rovide con-\\ntextually relevant answers. However, RAG is not without its draw-\\nbacks. The need for real-time retrieval introduces latency , while\\n∗Three authors contributed equally to this research.\\nKnowledge\\nRetrieval\\nModel\\nQ1\\nK1\\nQ2\\nK2\\nLLM\\nLLM\\nQ1K1\\nA1\\nQ2K2\\nA2\\nLLM\\nKnowledge\\nCache\\nPre-compute\\nQ1 Q2\\nLLM\\nKnowledge\\nCache\\nQ2\\nQ1\\nAppend Q1\\nA1\\nLLM\\nKnowledge\\nCache Q2\\nTruncate Q1\\nAppend Q2\\nA2\\nFigure 1: Comparison of Traditional RAG and our CAG\\nWorkﬂows: The upper section illustrates the RAG pipeline,\\nincluding real-time retrieval and reference text input dur -\\ning inference, while the lower section depicts our CAG ap-\\nproach, which preloads the KV-cache, eliminating the re-\\ntrieval step and reference text input at inference.\\nerrors in selecting or ranking relevant documents can degrade the\\nquality of the generated responses. Additionally, integra ting re-\\ntrieval and generation components increases system comple xity,\\nnecessitating careful tuning and adding to the maintenance over-\\nhead.\\nThis paper proposes an alternative paradigm, cache-augmen ted\\ngeneration (CAG), leveraging the capabilities of long-con text LLMs\\nto address these challenges. Instead of relying on a retriev al pipeline,\\nas shown in Figure 1, our approach involves preloading the LL M\\nwith all relevant documents in advance and precomputing the key-\\nvalue (KV) cache, which encapsulates the inference state of the\\n\\nQuestion: what is CAG?\\n\\nAnswer:   \", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\" Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know,\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:   \"\"\"\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"what is CAG?\")\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIyFC-GECjYl"
      },
      "source": [
        "# RAG_CHATBOT_QA_CHAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi1kSsdwBUR5",
        "outputId": "f28aa18a-b600-45a9-f848-12cfe8e6d925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAG, or Cache-Augmented Generation, is a framework that leverages the capabilities of long-context large language models (LLMs) to enable retrieval-free knowledge integration. Unlike traditional retrieval-augmented generation (RAG) systems, which rely on real-time retrieval of external knowledge during inference, CAG preloads relevant documents into the model and precomputes a key-value (KV) cache. This approach eliminates retrieval latency, reduces errors associated with document selection, and simplifies system complexity, making it a more streamlined and efficient alternative for managing knowledge-intensive workflows.\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
        "rag_chain = (\n",
        "    {\"context\":retriever | docs2str , \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "question = \"what is CAG?\"\n",
        "response = rag_chain.invoke(question)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvTijoEgDNFJ"
      },
      "source": [
        "# CONVERSATIONAL_QA_CHATBOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DyKg1wsvqlt"
      },
      "source": [
        "Saving the Chat History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zbj3AsO9DZ-k"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "chat_history = []\n",
        "chat_history.extend([HumanMessage(content=question), AIMessage(content=response)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq5qOUVQx4WX"
      },
      "source": [
        "Reformulating the question using chat history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AljV16kovvD3",
        "outputId": "568adbdd-15f6-4266-a54f-03f3ff433440"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'How is the Cache-Augmented Generation (CAG) framework implemented?'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given the chat history and the latest user question\"\n",
        "    \"which might reference context in the chat history,\"\n",
        "    \"formulate a standalone question which can be understood\"\n",
        "    \"without the chat history. Do Not answer the latest user question.\"\n",
        "    \"just reformulate it if needed and otherwise return as it is.\"\n",
        "\n",
        ")\n",
        "\n",
        "contextualize_q_prompt =  ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
        "\n",
        "contextualize_chain.invoke({\"input\": \"how is it implemented?\" , \"chat_history\": chat_history})\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Dc7fEkyLvG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ1q8mU00pCK",
        "outputId": "38cb970b-ad33-4301-92e4-3752fc6e8c49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'creationdate': 'D:20250321224934', 'creator': 'PDFium', 'page': 0, 'page_label': '1', 'producer': 'PDFium', 'source': '/content/docs/CAG.pdf', 'total_pages': 5}, page_content='arXiv:2412.15605v1  [cs.CL]  20 Dec 2024\\nDon’t Do RAG:\\nWhen Cache-Augmented Generation is All You Need for\\nKnowledge Tasks\\nBrian J Chan ∗\\nChao-Ting Chen∗\\nJui-Hung Cheng ∗\\nDepartment of Computer Science\\nNational Chengchi University\\nTaipei, Taiwan\\n{110703065,110703038,110703007}@nccu.edu.tw\\nHen-Hsen Huang\\nInsititue of Information Science\\nAcademia Sinica\\nTaipei, Taiwan\\nhhhuang@iis.sinica.edu.tw\\nAbstract\\nRetrieval-augmented generation (RAG) has gained tractionas a\\npowerful approach for enhancing language models by integra ting\\nexternal knowledge sources. However, RAG introduces chall enges\\nsuch as retrieval latency, potential errors in document sel ection,\\nand increased system complexity. With the advent of large la n-\\nguage models (LLMs) featuring signiﬁcantly extended conte xt win-\\ndows, this paper proposes an alternative paradigm, cache-a ugmented\\ngeneration (CAG) that bypasses real-time retrieval. Our me thod in-\\nvolves preloading all relevant resources, especially when the docu-\\nments or knowledge for retrieval are of a limited and managea ble\\nsize, into the LLM’s extended context and caching its runtim e pa-\\nrameters. During inference, the model utilizes these prelo aded pa-\\nrameters to answer queries without additional retrieval st eps. Com-\\nparative analyses reveal that CAG eliminates retrieval lat ency and\\nminimizes retrieval errors while maintaining context rele vance. Per-\\nformance evaluations across multiple benchmarks highligh t sce-\\nnarios where long-context LLMs either outperform or comple ment\\ntraditional RAG pipelines. These ﬁndings suggest that, for certain\\napplications, particularly those with a constrained knowl edge base,\\nCAG provide a streamlined and eﬃcient alternative to RAG, ac hiev-\\ning comparable or superior results with reduced complexity .\\nCCS Concepts\\n•Computing methodologies → Discourse, dialogue and prag-\\nmatics; Natural language generation ; • Information systems\\n→ Specialized information retrieval .\\nKeywords'),\n",
              " Document(metadata={'creationdate': 'D:20250321224934', 'creator': 'PDFium', 'page': 0, 'page_label': '1', 'producer': 'PDFium', 'source': '/content/docs/CAG.pdf', 'total_pages': 5}, page_content='CCS Concepts\\n•Computing methodologies → Discourse, dialogue and prag-\\nmatics; Natural language generation ; • Information systems\\n→ Specialized information retrieval .\\nKeywords\\nLarge Language Models, Retrieval Augmented Generation, Retrieval-\\nFree Question Answering\\n1 Introduction\\nThe advent of retrieval-augmented generation (RAG) [1, 3] h as\\nsigniﬁcantly enhanced the capabilities of large language m odels\\n(LLMs) by dynamically integrating external knowledge sour ces. RAG\\nsystems have proven eﬀective in handling open-domain quest ions\\nand specialized tasks, leveraging retrieval pipelines to p rovide con-\\ntextually relevant answers. However, RAG is not without its draw-\\nbacks. The need for real-time retrieval introduces latency , while\\n∗Three authors contributed equally to this research.\\nKnowledge\\nRetrieval\\nModel\\nQ1\\nK1\\nQ2\\nK2\\nLLM\\nLLM\\nQ1K1\\nA1\\nQ2K2\\nA2\\nLLM\\nKnowledge\\nCache\\nPre-compute\\nQ1 Q2\\nLLM\\nKnowledge\\nCache\\nQ2\\nQ1\\nAppend Q1\\nA1\\nLLM\\nKnowledge\\nCache Q2\\nTruncate Q1\\nAppend Q2\\nA2\\nFigure 1: Comparison of Traditional RAG and our CAG\\nWorkﬂows: The upper section illustrates the RAG pipeline,\\nincluding real-time retrieval and reference text input dur -\\ning inference, while the lower section depicts our CAG ap-\\nproach, which preloads the KV-cache, eliminating the re-\\ntrieval step and reference text input at inference.\\nerrors in selecting or ranking relevant documents can degrade the\\nquality of the generated responses. Additionally, integra ting re-\\ntrieval and generation components increases system comple xity,\\nnecessitating careful tuning and adding to the maintenance over-\\nhead.\\nThis paper proposes an alternative paradigm, cache-augmen ted\\ngeneration (CAG), leveraging the capabilities of long-con text LLMs\\nto address these challenges. Instead of relying on a retriev al pipeline,\\nas shown in Figure 1, our approach involves preloading the LL M\\nwith all relevant documents in advance and precomputing the key-\\nvalue (KV) cache, which encapsulates the inference state of the')]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm,\n",
        "    retriever,\n",
        "    contextualize_q_prompt\n",
        "\n",
        ")\n",
        "\n",
        "history_aware_retriever.invoke({\"input\":\"how is it implemented?\",\"chat_history\": chat_history})\n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL9oCHaT2Axe"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that answer the user's question.\"),\n",
        "    (\"system\",\"Context:{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(\n",
        "    llm,\n",
        "    qa_prompt,\n",
        "\n",
        ")\n",
        "\n",
        "rag_chain = create_retrieval_chain(\n",
        "    history_aware_retriever,\n",
        "    question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1YncJlJ3Tnr",
        "outputId": "d18017cc-8afe-456e-d547-3b491c2221e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'how is it implemented?',\n",
              " 'chat_history': [HumanMessage(content='what is CAG?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='CAG, or Cache-Augmented Generation, is a framework that leverages the capabilities of long-context large language models (LLMs) to enable retrieval-free knowledge integration. Unlike traditional retrieval-augmented generation (RAG) systems, which rely on real-time retrieval of external knowledge during inference, CAG preloads relevant documents into the model and precomputes a key-value (KV) cache. This approach eliminates retrieval latency, reduces errors associated with document selection, and simplifies system complexity, making it a more streamlined and efficient alternative for managing knowledge-intensive workflows.', additional_kwargs={}, response_metadata={})],\n",
              " 'context': [Document(metadata={'creationdate': 'D:20250321224934', 'creator': 'PDFium', 'page': 0, 'page_label': '1', 'producer': 'PDFium', 'source': '/content/docs/CAG.pdf', 'total_pages': 5}, page_content='arXiv:2412.15605v1  [cs.CL]  20 Dec 2024\\nDon’t Do RAG:\\nWhen Cache-Augmented Generation is All You Need for\\nKnowledge Tasks\\nBrian J Chan ∗\\nChao-Ting Chen∗\\nJui-Hung Cheng ∗\\nDepartment of Computer Science\\nNational Chengchi University\\nTaipei, Taiwan\\n{110703065,110703038,110703007}@nccu.edu.tw\\nHen-Hsen Huang\\nInsititue of Information Science\\nAcademia Sinica\\nTaipei, Taiwan\\nhhhuang@iis.sinica.edu.tw\\nAbstract\\nRetrieval-augmented generation (RAG) has gained tractionas a\\npowerful approach for enhancing language models by integra ting\\nexternal knowledge sources. However, RAG introduces chall enges\\nsuch as retrieval latency, potential errors in document sel ection,\\nand increased system complexity. With the advent of large la n-\\nguage models (LLMs) featuring signiﬁcantly extended conte xt win-\\ndows, this paper proposes an alternative paradigm, cache-a ugmented\\ngeneration (CAG) that bypasses real-time retrieval. Our me thod in-\\nvolves preloading all relevant resources, especially when the docu-\\nments or knowledge for retrieval are of a limited and managea ble\\nsize, into the LLM’s extended context and caching its runtim e pa-\\nrameters. During inference, the model utilizes these prelo aded pa-\\nrameters to answer queries without additional retrieval st eps. Com-\\nparative analyses reveal that CAG eliminates retrieval lat ency and\\nminimizes retrieval errors while maintaining context rele vance. Per-\\nformance evaluations across multiple benchmarks highligh t sce-\\nnarios where long-context LLMs either outperform or comple ment\\ntraditional RAG pipelines. These ﬁndings suggest that, for certain\\napplications, particularly those with a constrained knowl edge base,\\nCAG provide a streamlined and eﬃcient alternative to RAG, ac hiev-\\ning comparable or superior results with reduced complexity .\\nCCS Concepts\\n•Computing methodologies → Discourse, dialogue and prag-\\nmatics; Natural language generation ; • Information systems\\n→ Specialized information retrieval .\\nKeywords'),\n",
              "  Document(metadata={'creationdate': 'D:20250321224934', 'creator': 'PDFium', 'page': 0, 'page_label': '1', 'producer': 'PDFium', 'source': '/content/docs/CAG.pdf', 'total_pages': 5}, page_content='CCS Concepts\\n•Computing methodologies → Discourse, dialogue and prag-\\nmatics; Natural language generation ; • Information systems\\n→ Specialized information retrieval .\\nKeywords\\nLarge Language Models, Retrieval Augmented Generation, Retrieval-\\nFree Question Answering\\n1 Introduction\\nThe advent of retrieval-augmented generation (RAG) [1, 3] h as\\nsigniﬁcantly enhanced the capabilities of large language m odels\\n(LLMs) by dynamically integrating external knowledge sour ces. RAG\\nsystems have proven eﬀective in handling open-domain quest ions\\nand specialized tasks, leveraging retrieval pipelines to p rovide con-\\ntextually relevant answers. However, RAG is not without its draw-\\nbacks. The need for real-time retrieval introduces latency , while\\n∗Three authors contributed equally to this research.\\nKnowledge\\nRetrieval\\nModel\\nQ1\\nK1\\nQ2\\nK2\\nLLM\\nLLM\\nQ1K1\\nA1\\nQ2K2\\nA2\\nLLM\\nKnowledge\\nCache\\nPre-compute\\nQ1 Q2\\nLLM\\nKnowledge\\nCache\\nQ2\\nQ1\\nAppend Q1\\nA1\\nLLM\\nKnowledge\\nCache Q2\\nTruncate Q1\\nAppend Q2\\nA2\\nFigure 1: Comparison of Traditional RAG and our CAG\\nWorkﬂows: The upper section illustrates the RAG pipeline,\\nincluding real-time retrieval and reference text input dur -\\ning inference, while the lower section depicts our CAG ap-\\nproach, which preloads the KV-cache, eliminating the re-\\ntrieval step and reference text input at inference.\\nerrors in selecting or ranking relevant documents can degrade the\\nquality of the generated responses. Additionally, integra ting re-\\ntrieval and generation components increases system comple xity,\\nnecessitating careful tuning and adding to the maintenance over-\\nhead.\\nThis paper proposes an alternative paradigm, cache-augmen ted\\ngeneration (CAG), leveraging the capabilities of long-con text LLMs\\nto address these challenges. Instead of relying on a retriev al pipeline,\\nas shown in Figure 1, our approach involves preloading the LL M\\nwith all relevant documents in advance and precomputing the key-\\nvalue (KV) cache, which encapsulates the inference state of the')],\n",
              " 'answer': \"The implementation of Cache-Augmented Generation (CAG) involves several key steps:\\n\\n1. **Preloading Relevant Resources**: All relevant documents or knowledge sources are identified and preloaded into the large language model (LLM) before inference. This process relies on the assumption that the set of documents is limited and manageable, allowing for efficient caching.\\n\\n2. **Precomputing Key-Value (KV) Cache**: The next step is to precompute a KV cache that encapsulates the inference state of the LLM with respect to the preloaded documents. This KV cache stores information that helps the model retrieve context quickly without needing to perform real-time document retrieval.\\n\\n3. **Incorporating Cached Knowledge During Inference**: During the inference phase, queries are processed by the LLM, which utilizes the preloaded KV cache. Instead of relying on external retrieval, the model directly accesses the cached knowledge to generate responses. This integration allows it to reference relevant information without the need for additional retrieval steps.\\n\\n4. **Handling Queries**: The model manages inputs by appending the current query to the context from the KV cache, enabling it to produce an answer that leverages both the query and the cached knowledge. The input context can be adjusted (or truncated) based on the specific requirements or limits of the model's context window.\\n\\n5. **Maintaining Context Relevance**: The system aims to keep contexts relevant by ensuring that the preloaded documents are pertinent to the expected queries, allowing the model to produce contextually appropriate responses.\\n\\nOverall, the CAG approach prioritizes efficiency and performance by using preloaded knowledge and minimizing the complexities of real-time retrieval, thus facilitating faster and more reliable generation of responses in knowledge tasks.\"}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"input\":\"how is it implemented?\",\"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlvyaSj63zlY"
      },
      "source": [
        "# MultiUser ChatBot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egCmadH89Mwj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imXKlMlE9NTq"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "DB_NAME = \"rag_app.db\"\n",
        "\n",
        "\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "\n",
        "def create_application_logs():\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
        "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    session_id TEXT,\n",
        "                    user_query TEXT,\n",
        "                    gpt_response TEXT,\n",
        "                    model TEXT,\n",
        "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response,model) VALUES (?,?,?,?)',\n",
        "                  (session_id,user_query,gpt_response,model)  )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_chat_history(session_id):\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
        "    messages = []\n",
        "    for row in cursor.fetchall():\n",
        "        messages.extend([\n",
        "            {\"role\": \"human\", \"content\": row['user_query']},\n",
        "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
        "        ])\n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "# Initialize database\n",
        "create_application_logs()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2cs5a2Q3336",
        "outputId": "7c180bf3-ee5b-4946-c44e-0a29eb902aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "Question: what is CAG?\n",
            "Answer: CAG, or Cache-Augmented Generation, is a novel approach that leverages long-context large language models (LLMs) to enable retrieval-free knowledge integration. Unlike traditional Retrieval-Augmented Generation (RAG) systems that dynamically integrate external knowledge sources through real-time retrieval, CAG preloads relevant documents into the model and precomputes a key-value (KV) cache. This eliminates the latency and complexities associated with real-time retrieval and allows for more efficient processing of manageable knowledge bases. The approach is particularly effective for knowledge-intensive workflows and aims to simplify the integration of external information in tasks that require contextual understanding.\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "session_id = str(uuid.uuid4())\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "question1 = \"what is CAG?\"\n",
        "answer1 = rag_chain.invoke({\"input\":question,\"chat_history\":chat_history}) ['answer']\n",
        "insert_application_logs(session_id,question1,answer1,\"gpt-4o-mini\")\n",
        "print(f\"Question: {question1}\\nAnswer: {answer1}\")\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-clE9GbCYI9S",
        "outputId": "0371bd91-d4bb-4115-ea0f-6a55e6a778a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'human', 'content': 'what is CAG?'}, {'role': 'ai', 'content': 'CAG, or Cache-Augmented Generation, is a novel approach that leverages long-context large language models (LLMs) to enable retrieval-free knowledge integration. Unlike traditional Retrieval-Augmented Generation (RAG) systems that dynamically integrate external knowledge sources through real-time retrieval, CAG preloads relevant documents into the model and precomputes a key-value (KV) cache. This eliminates the latency and complexities associated with real-time retrieval and allows for more efficient processing of manageable knowledge bases. The approach is particularly effective for knowledge-intensive workflows and aims to simplify the integration of external information in tasks that require contextual understanding.'}]\n",
            "Question: how is it implemented?\n",
            "Answer: The implementation of Cache-Augmented Generation (CAG) follows a specific workflow that involves several key steps:\n",
            "\n",
            "1. **Preloading Knowledge**: All relevant documents or knowledge sources are preloaded into the large language model (LLM). This is feasible when the size of the knowledge base is limited and manageable.\n",
            "\n",
            "2. **Key-Value (KV) Cache Computation**: The model precomputes the key-value (KV) cache, which stores the inference state and relevant parameters that the model will use during query processing. This involves encoding the documents into a format that the model can reference efficiently during inference.\n",
            "\n",
            "3. **Inference Phase**:\n",
            "   - During inference, instead of performing real-time retrieval, the model directly accesses the preloaded knowledge stored in the KV cache.\n",
            "   - When a query is made, the model uses the precomputed parameters to generate responses based on the preloaded documents. This involves appending the query to the KV cache and utilizing the cached information to produce an answer.\n",
            "\n",
            "4. **Context Management**: The model may manage the context window effectively by truncating or appending documents based on the query, ensuring that the most relevant information is considered while generating responses.\n",
            "\n",
            "5. **Performance Evaluation**: The CAG method is evaluated against benchmarks to assess its performance compared to traditional RAG approaches, focusing on factors like response accuracy, latency, and system complexity.\n",
            "\n",
            "Overall, the CAG approach streamlines the process of integrating knowledge into language model responses by removing the retrieval step and relying on precomputed information, which can lead to faster and more efficient answer generation.\n"
          ]
        }
      ],
      "source": [
        "question2 = \"how is it implemented?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer2 = rag_chain.invoke({\"input\":question2,\"chat_history\":chat_history}) ['answer']\n",
        "insert_application_logs(session_id,question2,answer2,\"gpt-4o-mini\")\n",
        "print(f\"Question: {question2}\\nAnswer: {answer2}\")\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZRMVPsBZje2",
        "outputId": "e62c5533-1887-4d77-8283-f3d258371e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'human', 'content': 'what is CAG?'}, {'role': 'ai', 'content': 'CAG, or Cache-Augmented Generation, is a novel approach that leverages long-context large language models (LLMs) to enable retrieval-free knowledge integration. Unlike traditional Retrieval-Augmented Generation (RAG) systems that dynamically integrate external knowledge sources through real-time retrieval, CAG preloads relevant documents into the model and precomputes a key-value (KV) cache. This eliminates the latency and complexities associated with real-time retrieval and allows for more efficient processing of manageable knowledge bases. The approach is particularly effective for knowledge-intensive workflows and aims to simplify the integration of external information in tasks that require contextual understanding.'}, {'role': 'human', 'content': 'how is it implemented?'}, {'role': 'ai', 'content': 'The implementation of Cache-Augmented Generation (CAG) follows a specific workflow that involves several key steps:\\n\\n1. **Preloading Knowledge**: All relevant documents or knowledge sources are preloaded into the large language model (LLM). This is feasible when the size of the knowledge base is limited and manageable.\\n\\n2. **Key-Value (KV) Cache Computation**: The model precomputes the key-value (KV) cache, which stores the inference state and relevant parameters that the model will use during query processing. This involves encoding the documents into a format that the model can reference efficiently during inference.\\n\\n3. **Inference Phase**:\\n   - During inference, instead of performing real-time retrieval, the model directly accesses the preloaded knowledge stored in the KV cache.\\n   - When a query is made, the model uses the precomputed parameters to generate responses based on the preloaded documents. This involves appending the query to the KV cache and utilizing the cached information to produce an answer.\\n\\n4. **Context Management**: The model may manage the context window effectively by truncating or appending documents based on the query, ensuring that the most relevant information is considered while generating responses.\\n\\n5. **Performance Evaluation**: The CAG method is evaluated against benchmarks to assess its performance compared to traditional RAG approaches, focusing on factors like response accuracy, latency, and system complexity.\\n\\nOverall, the CAG approach streamlines the process of integrating knowledge into language model responses by removing the retrieval step and relying on precomputed information, which can lead to faster and more efficient answer generation.'}]\n",
            "Question: why is it important?\n",
            "Answer: Cache-Augmented Generation (CAG) is important for several reasons:\n",
            "\n",
            "1. **Reduced Latency**: By eliminating the need for real-time retrieval, CAG significantly decreases response times. This is particularly crucial in applications requiring quick interactions, such as chatbots, virtual assistants, and customer service systems, where users expect prompt replies.\n",
            "\n",
            "2. **Increased System Robustness**: Relying on precomputed key-value caches reduces the chances of errors that can occur during document retrieval and ranking in traditional RAG systems. CAG minimizes the complexity of retrieving and integrating external information, which can enhance the robustness and reliability of the system.\n",
            "\n",
            "3. **Efficiency for Manageable Knowledge Bases**: CAG is particularly effective when dealing with limited and well-defined knowledge bases. In scenarios where the relevant information is not vast and can fit into the model’s context window, CAG optimizes performance without the overhead of constant retrieval.\n",
            "\n",
            "4. **Simplification of the Architecture**: CAG simplifies the architecture of knowledge integration systems by removing the retrieval component. This leads to easier maintenance and tuning, reducing the overall complexity of the system, which can enable faster development cycles and deployment.\n",
            "\n",
            "5. **Enhanced Performance for Specific Applications**: The experiments and findings suggest that CAG can outperform traditional RAG systems in targeted applications, allowing organizations to leverage LLMs' capabilities more effectively in niche use cases, such as specialized information retrieval.\n",
            "\n",
            "6. **Practical Insights for Developers**: CAG provides actionable insights into optimizing workflows for knowledge-intensive tasks. It encourages researchers and developers to explore retrieval-free methods and understand when they may be more advantageous than traditional approaches.\n",
            "\n",
            "Overall, the importance of CAG lies in its ability to enhance the efficiency, speed, and reliability of applications that integrate knowledge with language models, contributing to improved user experiences and broader applicability of LLM technology.\n"
          ]
        }
      ],
      "source": [
        "question3 = \"why is it important?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer3 = rag_chain.invoke({\"input\":question3,\"chat_history\":chat_history}) ['answer']\n",
        "insert_application_logs(session_id,question3,answer3,\"gpt-4o-mini\")\n",
        "print(f\"Question: {question3}\\nAnswer: {answer3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OvXHYqAYmdZ"
      },
      "source": [
        "New User or New Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57uPdmxgYW0R"
      },
      "outputs": [],
      "source": [
        "session_id = str(uuid.uuid4())\n",
        "question = \"what is CAG?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer = rag_chain.invoke({\"input\":question,\"chat_history\":chat_history}) ['answer']\n",
        "insert_application_logs(session_id,question,answer,\"gpt-4o-mini\")\n",
        "print(f\"Question: {question}\\nAnswer: {answer}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
